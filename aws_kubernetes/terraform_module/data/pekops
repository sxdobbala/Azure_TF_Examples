#!/bin/bash
echo "#####  #####  #    # ###### #####   ####"
echo "#    # #      #  #   #    # #    # #"
echo "#####  #####  ##     #    # #####   ####"
echo "#      #      #  #   #    # #           #"
echo "#      #####  #    # ###### #       ####"
# PEKOPS - Product Engineering Kubernetes operations
# Kickstart script to set up the state for your Kubernetes cluster


set -o errexit
set -o nounset
set -o pipefail

AWS_REGION_FOR_S3_BUCKET=us-east-1
TEMPLATE_LAUNCH_CONFIGURATION_MASTER="aws_launch_configuration_master_template"
TEMPLATE_LAUNCH_CONFIGURATION_NODE="aws_launch_configuration_nodes_template"
TEMPLATE_INSTANCE_GROUP_MASTER="s3_template/instancegroup/master-template"
TEMPLATE_INSTANCE_GROUP_NODE="s3_template/instancegroup/nodes-template"
TEMPLATE_ADDONS="s3_template/addons/"
TEMPLATE_SECRETS="s3_template/secrets/"
TEMPLATE_KUBECONFIG="s3_template/kubeconfig"
TEMPLATE_CLUSTER_SPEC="s3_template/cluster.spec"
TEMPLATE_CLUSTER_CONFIG="s3_template/config"
TEMPLATE_TERRAFORM_CONFIG="t-terraform.tfvars"
TEMPLATE_OPEN_SSL_MASTER="t-openssl-master.cnf"
TEMPLATE_KEYSET_PUBLIC="s3_template/keyset_public.yaml"
TEMPLATE_KEYSET_PRIVATE="s3_template/keyset_private.yaml"

if [ "$(uname)" == "Darwin" ];then
	supersed="sed -i .bak"
else
	supersed="sed -i"
fi

function printhelp() {
 echo "Usage:"
 echo "************************************************************************************************"
 echo "    pekops -h                                            Display this help message"
 echo "    pekops setup                                         Set up the assests for the cluster and uploads to s3"
 echo "                 -c <clusterName>                        no default, must express"
 echo "                 -z <availabilityZoneCommaSeperatedList> default us-east-1a,us-east-1b,us-east-1c "
 echo "                 -v <awsVpcId>                           no default, must express" 
 echo "                 -m <masterCount>                        default 3"
 echo "                 -n <nodeCountMax>                       default 3"
 echo "                 -q <nodeCountMin>                       default 3"
 echo "                 -x <masterEC2InstanceType>              default t2.medium"
 echo "                 -y <nodeEC2InstanceType>                default t2.medium"
 echo "                 -o <osType>                             default is ubuntu,other option is amzn2"
 echo "                 -i <amiFilter>                          optional,default is *"
 echo "                 -f <amiOwner>                           optional,default is 757541135089"
 echo "                 -s <s3BucketName>                       no default, must express"
 echo "                 -r <route53ZoneId>                      no default, must express"
 echo "                 -e <egressProxy>                        no default, must express"
 echo "                 -t <tagNameIdentifier>                  default is "pe" must be same has one used in vpc"
 echo "    pekops clean                                         clean the workspace."
 echo "                 -c <clusterName>                        no default, must express"
 echo "                 -s <s3BucketName>                       no default, must express"
 echo "                 -r <route53ZoneId>                      no default, must express"
 echo "                 -p <aws_credentials_profile>            default is default"
 echo "                 -a <public_subnets>                     no default, must express"
 echo "                 -b <private_subnets>                    no default, must express"
 echo "************************************************************************************************"
 exit 1 
}

function validate_provisioner_policy(){
 local user_arn=`aws opsworks describe-my-user-profile --query 'UserProfile.IamUserArn' --output text 2>/dev/null` && \
 aws iam simulate-principal-policy --policy-source-arn ${user_arn} --action-names s3:* >/dev/null 2>&1 || USER_ACCESS=$? 
 if [[ ${USER_ACCESS:-unset} != "unset" ]]; then
 	echo "Unable to validate user access"
        echo "User must have \"opsworks:describe-my-user-profile\" and \"iam:simulate-principal-policy\" polices to validate access"
        while true; do
        	read -p "Do you wish to continue without validating user access? [yes/no]  :" yn
        	case $yn in
        	[Yy]* ) return;;
        	[Nn]* ) exit 0;;
        	* ) echo "Please answer yes or no.";;
        	esac
        done
 fi
 deny_count=`aws iam simulate-principal-policy --policy-source-arn ${user_arn} --action-names \
 s3:* ec2:* elasticloadbalancing:* autoscaling:* route53:* route53domains:* \
 iam:AddRoleToInstanceProfile iam:CreateInstanceProfile iam:CreateRole iam:CreatePolicy ssm:CreateDocument \
 --query 'EvaluationResults[*].EvalDecision' --output json|grep -c "Deny"` || true 
 if [ $deny_count -eq 0 ];then
	 echo "$(t1me) successfully validated user IAM policy"
 else
	 echo "\"$user_arn\" do not have all the access needed to run the Kubernetes cluster"
 	 aws iam simulate-principal-policy --policy-source-arn ${user_arn} --action-names \
	 s3:* ec2:* elasticloadbalancing:* autoscaling:* route53:* route53domains:* \
	 iam:AddRoleToInstanceProfile iam:CreateInstanceProfile iam:CreateRole iam:CreatePolicy ssm:CreateDocument \
 	 --query 'EvaluationResults[*].[EvalActionName,EvalDecision]' --output table
 	 exit 1 
 fi
}

function validate_vpc(){
 valid_subnet_count=${#AWS_PRIVATE_SUBNETS[@]}
 if [ $MASTER_COUNT -gt $valid_subnet_count ]; then
	 echo "-----------------ERROR--------------------"
	 echo "Something is not right with VPC id provided"
	 echo "Either it is not valid or it is not set up in specified availability zone or it does not have the Subnets setup properly"
	 echo "Refer repo CommercialCloud-EAC/aws_vpc to find how to setup VPC"
	 exit 1
 fi
}

function get_az_list(){
 local count=0
 let t=${#AWS_ZONES[@]}
 let m=${#AWS_ZONES[@]}-1 || true
 while [ $count -lt $t ]
 do
	 if [ $count == 0 ]; then
		 LIST=[" "\"${AWS_ZONES[$count]}\"
	 else
		 LIST=${LIST}\"${AWS_ZONES[$count]}\"
	 fi
	 if [ $count != $m ]; then
		 LIST=${LIST},
	 fi
	 LIST=${LIST}" "
	 let count=count+1
 done
 ZONE_LIST=${LIST}]
}

function get_etcd_map(){
 local count=0
 alpha=({a..z})
 let t=$MASTER_COUNT
 let m=${MASTER_COUNT}-1 || true
 while [ $count -lt $t ]
 do
	 if [ $count == 0 ]; then
		 MAP=[" "{\"instanceGroup\":\"master-${AWS_ZONES[$count]}\",\"name\":\"${alpha[$count]}\"}
	 else
		 MAP=${MAP}{\"instanceGroup\":\"master-${AWS_ZONES[$count]}\",\"name\":\"${alpha[$count]}\"}
	 fi
	 if [ $count != $m ]; then
		 MAP=${MAP},
	 fi
	 MAP=${MAP}" "
	 let count=count+1
 done
 ETCD_INSTANCE_GROUP=${MAP}]
}

function get_etcd_list(){
 local count=0
 alpha=({a..z})
 let t=$MASTER_COUNT
 let m=${MASTER_COUNT}-1 || true
 local LIST
 while [ $count -lt $t ]
 do
	 if [ $count == 0 ]; then
		 LIST=\"http:\\/\\/etcd-${alpha[$count]}.internal.$CLUSTER_NAME:4001
	 else
		 LIST=${LIST}http:\\/\\/etcd-${alpha[$count]}.internal.$CLUSTER_NAME:4001
	 fi
	 if [ $count != $m ]; then
		 LIST=${LIST},
	 fi
	 let count=count+1
 done
 ETCD_LIST=${LIST}\"
}

function get_subnet_list(){
 local count=0
 let t=$MASTER_COUNT
 let m=${MASTER_COUNT}-1 || true
 local LIST
 local sub_array
 while [ $count -lt $t ]
 do     
        sub_array=($(aws ec2 describe-subnets --subnet-ids ${AWS_PRIVATE_SUBNETS[$count]} --query 'Subnets[].[AvailabilityZone,CidrBlock]' --output text --profile $AWS_PROFILE))|| (echo "error occuered,invalid subnetid provided" && exit 1)
 	if [ $count == 0 ]; then
		LIST="{\"cidr\": \"${sub_array[1]/\//\\/}\",\"name\": \"${sub_array[0]}\",\"type\": \"Private\",\"zone\": \"${sub_array[0]}\",\"id\": \"${AWS_PRIVATE_SUBNETS[$count]}\"}"
 	else
 		LIST="${LIST}{\"cidr\": \"${sub_array[1]/\//\\/}\",\"name\": \"${sub_array[0]}\",\"type\": \"Private\",\"zone\": \"${sub_array[0]}\",\"id\": \"${AWS_PRIVATE_SUBNETS[$count]}\"}"
 	fi
 	if [ $count != $m ]; then
 		LIST=${LIST},
 	fi
 	LIST=${LIST}" "
 	let count=count+1
 done
 count=0
 while [ $count -lt $t ]
 do
        sub_array=($(aws ec2 describe-subnets --subnet-ids ${AWS_PUBLIC_SUBNETS[$count]} --query 'Subnets[].[AvailabilityZone,CidrBlock]' --output text --profile $AWS_PROFILE))|| (echo "error occuered,invalid subnetid provided" && exit 1)
        if [ $count == 0 ]; then
                LIST="${LIST},{\"cidr\": \"${sub_array[1]/\//\\/}\",\"name\": \"utility-${sub_array[0]}\",\"type\": \"Utility\",\"zone\": \"${sub_array[0]}\",\"id\": \"${AWS_PUBLIC_SUBNETS[$count]}\"}"
        else
                LIST="${LIST}{\"cidr\": \"${sub_array[1]/\//\\/}\",\"name\": \"utility-${sub_array[0]}\",\"type\": \"Utility\",\"zone\": \"${sub_array[0]}\",\"id\": \"${AWS_PUBLIC_SUBNETS[$count]}\"}"
        fi
        if [ $count != $m ]; then
                LIST=${LIST},
        fi
        LIST=${LIST}" "
        let count=count+1
 done
 SUBNET_LIST=${LIST}
}

function get_vpc_cidr(){
 cidr=$(aws ec2 describe-vpcs --query 'Vpcs[?VpcId==`'$VPC'`].CidrBlock' --output text)
 VPC_CIDR=${cidr%%/*}\\/${cidr#*/}
}

function validate_ami(){
 local image_owner=${AMI_OWNER}
 local image_name_filter="*${OS_TYPE}*${AMI_IMAGE_FILTER}*"
 local image_name=`aws ec2 describe-images --filters Name=owner-id,Values=${image_owner} Name=name,Values=${image_name_filter} Name=root-device-type,Values=ebs\
 --query 'sort_by(Images, &CreationDate) | [-1].Name' --output text`
 if [ $image_name != "None" ]; then
 	IMAGE_OWNER=$image_owner
 	IMAGE_NAME=${image_name//\//\\/}
 	IMAGE_SOURCE=${image_owner}\\/${image_name//\//\\/}
 else
 echo "-----------------ERROR--------------------"
 echo "Something is not right with the AMI provided"
 exit 1 
 fi
}

#There is no easy way for Terraform to add the tags to existing resources to address the issue #14 
function add_subnet_tags(){
 pub_sub_tags=($(aws ec2 describe-tags --filters "Name=resource-id,Values=${PUBLIC_SUBNETS[@]}" "Name=key,Values=kubernetes.io/cluster/$CLUSTER_NAME" --query 'Tags[].Value' --output text --profile $AWS_PROFILE))
 pri_sub_tags=($(aws ec2 describe-tags --filters "Name=resource-id,Values=${PRIVATE_SUBNETS[@]}" "Name=key,Values=kubernetes.io/cluster/$CLUSTER_NAME" --query 'Tags[].Value' --output text --profile $AWS_PROFILE))
 if [[ ${#pub_sub_tags[@]} == ${#AWS_PUBLIC_SUBNETS[@]} && ${pub_sub_tags[0]} == "shared" ]]; then
 	echo "$(t1me) required tags are present in public subnets. No new tags will be added"
 else
 	echo "$(t1me) missing tags in public subnets. Tags will be added"
        aws ec2 create-tags --resources ${AWS_PUBLIC_SUBNETS[@]} --tags Key="kubernetes.io/cluster/$CLUSTER_NAME",Value="shared" Key="kubernetes.io/role/elb",Value="1"
 fi
 if [[ ${#pri_sub_tags[@]} == ${#AWS_PRIVATE_SUBNETS[@]} && ${pri_sub_tags[0]} == "shared" ]]; then
 	echo "$(t1me) required tags are present in private subnets. No new tags will be added"
 else
 	echo "$(t1me) missing tags in private subnets. Tags will be added"
        aws ec2 create-tags --resources ${AWS_PRIVATE_SUBNETS[@]} --tags Key="kubernetes.io/cluster/$CLUSTER_NAME",Value="shared" Key="kubernetes.io/role/internal-elb",Value="1"
 fi
}

function t1me(){
 date "+%Y.%m.%d-%H.%M.%S"
}

function easy_pki(){
 local myname=$(od -A n -t d -N 16 /dev/urandom | tr -dc '0-9' | cut -c1-28)
 mkdir -p $CLUSTER_NAME/pki/private/$2 && mkdir -p $CLUSTER_NAME/pki/issued/$2
 echo "$(t1me) Issuing new certificate: $2"
 openssl genrsa -out $CLUSTER_NAME/pki/private/$2/$myname.key 2048 2>/dev/null
 cp $TEMPLATE_KEYSET_PRIVATE $CLUSTER_NAME/pki/private/$2/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/${2}/g" $CLUSTER_NAME/pki/private/$2/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${myname}/g" $CLUSTER_NAME/pki/private/$2/keyset.yaml
 private_key_base64=`cat $CLUSTER_NAME/pki/private/$2/$myname.key|base64 -w 0`
 $supersed "s/KEYSET_PRIVATE_PLACE_HOLDER/${private_key_base64}/g" $CLUSTER_NAME/pki/private/$2/keyset.yaml
 openssl req -new -key $CLUSTER_NAME/pki/private/$2/$myname.key -out $CLUSTER_NAME/pki/private/$2/$myname.csr -config $1 -subj "$3" 2>/dev/null
 openssl x509 -req -in $CLUSTER_NAME/pki/private/$2/$myname.csr -CA $CLUSTER_NAME/pki/issued/ca/*.crt \
 -CAkey $CLUSTER_NAME/pki/private/ca/*.key -CAcreateserial -out $CLUSTER_NAME/pki/issued/$2/$myname.crt \
 -days 3650 -extensions v3_ca -extfile $1 2>/dev/null
 cp $TEMPLATE_KEYSET_PUBLIC $CLUSTER_NAME/pki/issued/$2/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/${2}/g" $CLUSTER_NAME/pki/issued/$2/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${myname}/g" $CLUSTER_NAME/pki/issued/$2/keyset.yaml
 public_key_base64=`cat $CLUSTER_NAME/pki/issued/$2/$myname.crt|base64 -w 0`
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/issued/$2/keyset.yaml
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/private/$2/keyset.yaml
 rm -rf $CLUSTER_NAME/pki/private/$2/$myname.csr  $CLUSTER_NAME/pki/private/ca/*.srl
}

function create_launch_script_master() {
 echo "$(t1me) creating master launch scripts"
 local count=0
 if [ -d "$CLUSTER_NAME" ]; then
        rm -rf $CLUSTER_NAME && mkdir $CLUSTER_NAME
 else
        mkdir $CLUSTER_NAME
 fi
 mkdir -p $CLUSTER_NAME/launch_script
 while [ $count -lt $MASTER_COUNT ]
 do
 	cp $TEMPLATE_LAUNCH_CONFIGURATION_MASTER $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
        $supersed "s/EGRESS_PROXY_PLACE_HOLDER/${EGRESS_PROXY}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
 	$supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
        $supersed "s/VPC_CIDR_PLACE_HOLDER/${VPC_CIDR}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
 	$supersed "s/S3_STORE_BUCKET_NAME_PLACE_HOLDER/${S3_STORE_BUCKET_NAME}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
 	$supersed "s/ZONE_PLACE_HOLDER/${AWS_ZONES[$count]}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_master-${AWS_ZONES[$count]}.${CLUSTER_NAME}_user_data${SELF_ID}
 	let count=count+1
 done
}

function create_launch_script_nodes() {
 echo "$(t1me) creating nodes launch script"
 cp $TEMPLATE_LAUNCH_CONFIGURATION_NODE  $CLUSTER_NAME/launch_script/aws_launch_configuration_nodes.${CLUSTER_NAME}_user_data${SELF_ID}
 $supersed "s/EGRESS_PROXY_PLACE_HOLDER/${EGRESS_PROXY}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_nodes.${CLUSTER_NAME}_user_data${SELF_ID}
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_nodes.${CLUSTER_NAME}_user_data${SELF_ID}
 $supersed "s/VPC_CIDR_PLACE_HOLDER/${VPC_CIDR}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_nodes.${CLUSTER_NAME}_user_data${SELF_ID}
 $supersed "s/S3_STORE_BUCKET_NAME_PLACE_HOLDER/${S3_STORE_BUCKET_NAME}/g" $CLUSTER_NAME/launch_script/aws_launch_configuration_nodes.${CLUSTER_NAME}_user_data${SELF_ID}
}

function manage_addons() {
 echo "$(t1me) adding k8s addons for your cluster"
 cp -R $TEMPLATE_ADDONS $CLUSTER_NAME/addons/
 $supersed "s/ETCD_ENDPOINTS_PLACE_HOLDER/$ETCD_LIST/g" $CLUSTER_NAME/addons/networking.projectcalico.org/k8s-1.6.yaml
 $supersed "s/ETCD_ENDPOINTS_PLACE_HOLDER/$ETCD_LIST/g" $CLUSTER_NAME/addons/networking.projectcalico.org/k8s-1.7.yaml
 $supersed "s/ETCD_ENDPOINTS_PLACE_HOLDER/$ETCD_LIST/g" $CLUSTER_NAME/addons/networking.projectcalico.org/pre-k8s-1.6.yaml
 $supersed "s/HOSTED_ZONE_PLACE_HOLDER/$ROUTE53_ZONEID/g" $CLUSTER_NAME/addons/dns-controller.addons.k8s.io/k8s-1.6.yaml
 $supersed "s/HOSTED_ZONE_PLACE_HOLDER/$ROUTE53_ZONEID/g" $CLUSTER_NAME/addons/dns-controller.addons.k8s.io/pre-k8s-1.6.yaml
 $supersed "s/EGRESS_PROXY_PLACE_HOLDER/${EGRESS_PROXY}/g" $CLUSTER_NAME/addons/dns-controller.addons.k8s.io/k8s-1.6.yaml
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/addons/dns-controller.addons.k8s.io/k8s-1.6.yaml
 $supersed "s/VPC_CIDR_PLACE_HOLDER/${VPC_CIDR}/g" $CLUSTER_NAME/addons/dns-controller.addons.k8s.io/k8s-1.6.yaml
 #future implementation for supporting other versions & plugins(ex:network plugin)
}

function manage_instancegroup_master() {
 echo "$(t1me) creating master instance group"
 mkdir -p $CLUSTER_NAME/instancegroup
 local count=0
 while [ $count -lt $MASTER_COUNT ]
 do
 	cp $TEMPLATE_INSTANCE_GROUP_MASTER $CLUSTER_NAME/instancegroup/master-${AWS_ZONES[$count]}
 	$supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/instancegroup/master-${AWS_ZONES[$count]}
 	$supersed "s/IMAGE_PLACE_HOLDER/${IMAGE_SOURCE}/g" $CLUSTER_NAME/instancegroup/master-${AWS_ZONES[$count]}
	$supersed "s/MASTER_INSTANCE_TYPE_PLACE_HOLDER/${MASTER_INSTANCE}/g" $CLUSTER_NAME/instancegroup/master-${AWS_ZONES[$count]}
 	$supersed "s/ZONE_PLACE_HOLDER/${AWS_ZONES[$count]}/g" $CLUSTER_NAME/instancegroup/master-${AWS_ZONES[$count]}
	let count=count+1
 done
}

function manage_instancegroup_nodes() {
 echo "$(t1me) creating nodes instance group"
 cp $TEMPLATE_INSTANCE_GROUP_NODE $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/IMAGE_PLACE_HOLDER/${IMAGE_SOURCE}/g" $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/NODE_INSTANCE_TYPE_PLACE_HOLDER/${NODE_INSTANCE}/g" $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/MAX_NODE_COUNT_PLACE_HOLDER/${NODE_COUNT_MAX}/g" $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/MIN_NODE_COUNT_PLACE_HOLDER/${NODE_COUNT_MIN}/g" $CLUSTER_NAME/instancegroup/nodes
 $supersed "s/ALL_ZONE_PLACE_HOLDER/${ZONE_LIST}/g" $CLUSTER_NAME/instancegroup/nodes
}

function manage_pki() {
 echo "$(t1me) managing pki for your cluster"
 #cp -R $TEMPLATE_PKI $CLUSTER_NAME/
 local myname=$(od -A n -t d -N 16 /dev/urandom | tr -dc '0-9' | cut -c1-28)
 #creating rootCA
 echo "$(t1me) creating new certificate authority"
 mkdir -p $CLUSTER_NAME/pki/private/ca && mkdir -p $CLUSTER_NAME/pki/issued/ca
 openssl genrsa -out $CLUSTER_NAME/pki/private/ca/$myname.key 2048 2>/dev/null
 cp $TEMPLATE_KEYSET_PRIVATE $CLUSTER_NAME/pki/private/ca/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/ca/g" $CLUSTER_NAME/pki/private/ca/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${myname}/g" $CLUSTER_NAME/pki/private/ca/keyset.yaml
 private_key_base64=`cat $CLUSTER_NAME/pki/private/ca/$myname.key|base64 -w 0`
 $supersed "s/KEYSET_PRIVATE_PLACE_HOLDER/${private_key_base64}/g" $CLUSTER_NAME/pki/private/ca/keyset.yaml
 openssl req -x509 -new -nodes -key $CLUSTER_NAME/pki/private/ca/$myname.key -sha256 -days 3650 \
 -out $CLUSTER_NAME/pki/issued/ca/$myname.crt -subj "//CN=kuberentes" 2>/dev/null
 cp $TEMPLATE_KEYSET_PUBLIC $CLUSTER_NAME/pki/issued/ca/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/ca/g" $CLUSTER_NAME/pki/issued/ca/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${myname}/g" $CLUSTER_NAME/pki/issued/ca/keyset.yaml
 public_key_base64=`cat $CLUSTER_NAME/pki/issued/ca/$myname.crt|base64 -w 0`
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/issued/ca/keyset.yaml
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/private/ca/keyset.yaml
 cp $TEMPLATE_OPEN_SSL_MASTER openssl-master.cnf
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" openssl-master.cnf
 easy_pki openssl-master.cnf master "//CN=kubernetes-master"
 easy_pki openssl-kubecfg.cnf kubecfg "/O=system:masters/CN=kubecfg"
 easy_pki openssl-kubelet.cnf kubelet "/O=system:nodes/CN=kubelet"
 easy_pki openssl-scheduler.cnf kube-scheduler "/CN=system:kube-scheduler"
 easy_pki openssl-proxy.cnf kube-proxy "/CN=system:kube-proxy"
 easy_pki openssl-manager.cnf kube-controller-manager "/CN=system:kube-controller-manager"
 easy_pki openssl-kops.cnf kops "/O=system:masters/CN=kops"
 easy_pki openssl-kubelet-api.cnf kubelet-api "/CN=kubelet-api"
 easy_pki openssl-apiserver-proxy-client.cnf apiserver-proxy-client "/CN=apiserver-proxy-client"
 echo "$(t1me) creating new certificate authority for apiserver aggregator"
 local newname=$(od -A n -t d -N 16 /dev/urandom | tr -dc '0-9' | cut -c1-28)
 mkdir -p $CLUSTER_NAME/pki/private/apiserver-aggregator-ca && mkdir -p $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca
 openssl genrsa -out $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/$newname.key 2048 2>/dev/null
 cp $TEMPLATE_KEYSET_PRIVATE $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/apiserver-aggregator-ca/g" $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${newname}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/keyset.yaml
 private_key_base64=`cat $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/$newname.key|base64 -w 0`
 $supersed "s/KEYSET_PRIVATE_PLACE_HOLDER/${private_key_base64}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/keyset.yaml
 openssl req -x509 -new -nodes -key $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/$newname.key -sha256 -days 3650 \
 -out $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/$newname.crt -subj "//CN=apiserver-aggregator-ca" 2>/dev/null
 cp $TEMPLATE_KEYSET_PUBLIC $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/apiserver-aggregator-ca/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${newname}/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/keyset.yaml
 public_key_base64=`cat $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/$newname.crt|base64 -w 0`
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/keyset.yaml
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/keyset.yaml
 echo "$(t1me) Issuing new certificate: apiserver-aggregator"
 local mypetname=$(od -A n -t d -N 16 /dev/urandom | tr -dc '0-9' | cut -c1-28)
 mkdir -p $CLUSTER_NAME/pki/private/apiserver-aggregator && mkdir -p $CLUSTER_NAME/pki/issued/apiserver-aggregator
 openssl genrsa -out $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.key 2048 2>/dev/null
 cp $TEMPLATE_KEYSET_PRIVATE $CLUSTER_NAME/pki/private/apiserver-aggregator/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/apiserver-aggregator/g" $CLUSTER_NAME/pki/private/apiserver-aggregator/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${mypetname}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator/keyset.yaml
 private_key_base64=`cat $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.key|base64 -w 0`
 $supersed "s/KEYSET_PRIVATE_PLACE_HOLDER/${private_key_base64}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator/keyset.yaml
 openssl req -new -key $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.key \
 -out $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.csr -config openssl-apiserver-aggregator.cnf -subj "/CN=aggregator" 2>/dev/null
 openssl x509 -req -in $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.csr -CA $CLUSTER_NAME/pki/issued/apiserver-aggregator-ca/*.crt \
 -CAkey $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/*.key -CAcreateserial -out $CLUSTER_NAME/pki/issued/apiserver-aggregator/$mypetname.crt \
 -days 3650 -extensions v3_ca -extfile openssl-apiserver-aggregator.cnf 2>/dev/null
 cp $TEMPLATE_KEYSET_PUBLIC $CLUSTER_NAME/pki/issued/apiserver-aggregator/keyset.yaml
 $supersed "s/KEYSET_NAME_PLACE_HOLDER/apiserver-aggregator/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator/keyset.yaml
 $supersed "s/KEYSET_ID_PLACE_HOLDER/${mypetname}/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator/keyset.yaml
 public_key_base64=`cat $CLUSTER_NAME/pki/issued/apiserver-aggregator/$mypetname.crt|base64 -w 0`
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/issued/apiserver-aggregator/keyset.yaml
 $supersed "s/KEYSET_PUBLIC_PLACE_HOLDER/${public_key_base64}/g" $CLUSTER_NAME/pki/private/apiserver-aggregator/keyset.yaml
 rm -rf $CLUSTER_NAME/pki/private/apiserver-aggregator/$mypetname.csr  $CLUSTER_NAME/pki/private/apiserver-aggregator-ca/*.srl 
}

function manage_secrets() {
 echo "$(t1me) adding secrets for your cluster"
 cp -R $TEMPLATE_SECRETS $CLUSTER_NAME/secrets
 local ADMIN_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)"| base64`
 local KUBELET_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local KUBEPROXY_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local CONTROLLER_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local DNS_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local LOGGING_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local MONITORING_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 local SCHEDULER_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 KUBE_SECRET=`printf "$(dd bs=24 count=1 if=/dev/urandom status=none | base64 | tr +/ _a)" | base64` 
 $supersed "s/ADMIN_SECRET_PLACE_HOLDER/${ADMIN_SECRET}/g" $CLUSTER_NAME/secrets/admin
 $supersed "s/KUBELET_SECRET_PLACE_HOLDER/${KUBELET_SECRET}/g" $CLUSTER_NAME/secrets/kubelet
 $supersed "s/KUBEPROXY_SECRET_PLACE_HOLDER/${KUBEPROXY_SECRET}/g" $CLUSTER_NAME/secrets/kube-proxy
 $supersed "s/CONTROLLER_SECRET_PLACE_HOLDER/${CONTROLLER_SECRET}/g" $CLUSTER_NAME/secrets/system:controller_manager
 $supersed "s/DNS_SECRET_PLACE_HOLDER/${DNS_SECRET}/g" $CLUSTER_NAME/secrets/system:dns
 $supersed "s/LOGGING_SECRET_PLACE_HOLDER/${LOGGING_SECRET}/g" $CLUSTER_NAME/secrets/system:logging
 $supersed "s/MONITORING_SECRET_PLACE_HOLDER/${MONITORING_SECRET}/g" $CLUSTER_NAME/secrets/system:monitoring
 $supersed "s/SCHEDULER_SECRET_PLACE_HOLDER/${SCHEDULER_SECRET}/g" $CLUSTER_NAME/secrets/system:scheduler
 $supersed "s/KUBE_SECRET_PLACE_HOLDER/${KUBE_SECRET}/g"  $CLUSTER_NAME/secrets/kube
}

function manage_kubeconfig() {
 echo "$(t1me) creating kubeconfig for you cluster"
 cp $TEMPLATE_KUBECONFIG $CLUSTER_NAME/
 local CA_DATA=`cat $CLUSTER_NAME/pki/issued/ca/*.crt|base64|tr -d '[:space:]'`
 local CLIENT_CERTIFICATE=`cat $CLUSTER_NAME/pki/issued/kubecfg/*.crt|base64|tr -d '[:space:]'`
 local CLIENT_KEY=`cat $CLUSTER_NAME/pki/private/kubecfg/*.key|base64|tr -d '[:space:]'`
 local KUBE_PASSWORD=`printf "${KUBE_SECRET}"|base64 --decode`
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/kubeconfig
 $supersed "s/CERTIFICATE_AUTHORITY_DATA_PLACE_HOLDER/${CA_DATA}/g" $CLUSTER_NAME/kubeconfig
 $supersed "s/CLIENT_CERTIFICATE_DATA_PLACE_HOLDER/${CLIENT_CERTIFICATE}/g" $CLUSTER_NAME/kubeconfig
 $supersed "s/CLIENT_KEY_PLACE_HOLDER/${CLIENT_KEY}/g" $CLUSTER_NAME/kubeconfig
 $supersed "s/KUBE_PASSWORD_PLACE_HOLDER/${KUBE_PASSWORD}/g" $CLUSTER_NAME/kubeconfig
 [ -d ~/.kube ] || mkdir ~/.kube
 cp $CLUSTER_NAME/kubeconfig ~/.kube/
 rm -rf $CLUSTER_NAME/kubeconfig
}

function manage_cluster_spec() {
 echo "$(t1me) creating cluster specification"
 cp $TEMPLATE_CLUSTER_SPEC $CLUSTER_NAME/
 cp $TEMPLATE_CLUSTER_CONFIG $CLUSTER_NAME/
 $supersed "s/EGRESS_PROXY_HOST_PLACE_HOLDER/${EGRESS_PROXY%%:*}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/EGRESS_PROXY_PORT_PLACE_HOLDER/${EGRESS_PROXY#*:}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/S3_STORE_BUCKET_NAME_PLACE_HOLDER/${S3_STORE_BUCKET_NAME}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/DNS_ZONE_PLACE_HOLDER/${ROUTE53_ZONEID}/g" $CLUSTER_NAME/cluster.spec
 $supersed "s/INSTANCE_GROUP_PLACE_HOLDER/${ETCD_INSTANCE_GROUP}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/VPC_CIDR_PLACE_HOLDER/${VPC_CIDR}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/VPC_ID_PLACE_HOLDER/${VPC}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
 $supersed "s/SUBNET_LIST_PLACE_HOLDER/${SUBNET_LIST}/g" $CLUSTER_NAME/cluster.spec $CLUSTER_NAME/config
}

function update_terraform_user_config(){
 echo "$(t1me) updating terraform config for your kubernetes cluster"
 cp $TEMPLATE_TERRAFORM_CONFIG ../terraform.tfvars
 local AWS_REGION=${AWS_ZONES[0]:0:${#AWS_ZONES[0]}-1}
 $supersed "s/AWS_VPC_PLACE_HOLDER/${VPC}/g" ../terraform.tfvars
 $supersed "s/AZ_PLACE_HOLDER/${ZONE_LIST}/g" ../terraform.tfvars
 $supersed "s/AWS_REGION_PLACE_HOLDER/${AWS_REGION}/g" ../terraform.tfvars
 $supersed "s/AWS_PROFILE_PLACE_HOLDER/${AWS_PROFILE}/g" ../terraform.tfvars
 $supersed "s/K8S_CLUSTER_NAME_PLACE_HOLDER/${CLUSTER_NAME}/g" ../terraform.tfvars
 $supersed "s/K8S_MASTER_COUNT_PLACE_HOLDER/${MASTER_COUNT}/g" ../terraform.tfvars
 $supersed "s/K8S_NODE_COUNT_PLACE_HOLDER/${NODE_COUNT_MAX}/g" ../terraform.tfvars
 $supersed "s/EC2_NODE_INSTANCE_TYPE_PLACE_HOLDER/${NODE_INSTANCE}/g" ../terraform.tfvars
 $supersed "s/EC2_MASTER_INSTANCE_TYPE_PLACE_HOLDER/${MASTER_INSTANCE}/g" ../terraform.tfvars
 $supersed "s/K8S_ROUTE53_ZONE_PLACE_HOLDER/${ROUTE53_ZONEID}/g" ../terraform.tfvars
 $supersed "s/AMI_NAME_PLACE_HOLDER/${IMAGE_NAME}/g" ../terraform.tfvars
 $supersed "s/AMI_OWNER_PLACE_HOLDER/${IMAGE_OWNER}/g" ../terraform.tfvars
 $supersed "s/VPC_S3_ENDPOINT_PLACE_HOLDER/${ENABLE_VPC_S3_ENDPOINT}/g" ../terraform.tfvars
 $supersed "s/VPC_DYNAMODB_ENDPOINT_PLACE_HOLDER/${ENABLE_VPC_DYNAMODB_ENDPOINT}/g" ../terraform.tfvars
 $supersed "s/TAG_NAME_IDENTIFIER_PLACE_HOLDER/${TAG_NAME_IDENTIFIER}/g" ../terraform.tfvars
}

function create_s3_bucket_and_upload_assests() {
 echo "$(t1me) creating s3 bucket to store cluster configurations"
 if [ "$(uname)" == "Darwin" ];then
 	find `pwd`/* -name "*.bak" -exec rm {} +
 fi
 aws s3 ls $S3_STORE_BUCKET_NAME --profile $AWS_PROFILE &>/dev/null || \
 aws s3api create-bucket --bucket $S3_STORE_BUCKET_NAME --region us-east-1 --acl private --profile $AWS_PROFILE >/dev/null && sleep 10
 aws s3api put-bucket-encryption --bucket $S3_STORE_BUCKET_NAME --server-side-encryption-configuration \
 '{
    "Rules": [
        {
            "ApplyServerSideEncryptionByDefault": {
                "SSEAlgorithm": "AES256"
            }
        }
    ]
 }'
 echo "$(t1me) uploading assets to s3"
 count=`aws s3 ls s3://$S3_STORE_BUCKET_NAME/$CLUSTER_NAME --profile $AWS_PROFILE|wc -l`|| true
 if [[ $count -gt 0 ]]; then
 	aws s3 rm s3://$S3_STORE_BUCKET_NAME/$CLUSTER_NAME --recursive --profile $AWS_PROFILE --quiet && \
	aws s3 sync $CLUSTER_NAME/ s3://$S3_STORE_BUCKET_NAME/$CLUSTER_NAME/ --quiet --profile $AWS_PROFILE
 else
	aws s3 sync $CLUSTER_NAME/ s3://$S3_STORE_BUCKET_NAME/$CLUSTER_NAME/ --quiet --profile $AWS_PROFILE
 fi
}

function clean_s3_bucket() {
 echo "$(t1me) Removing s3 bucket with all cluster configuration objects"
 aws s3 rb s3://$S3_STORE_BUCKET_NAME/ --force --profile $AWS_PROFILE || echo "error occuered while removing the bucket" 
}

function clean_workspace() {
 echo "$(t1me) Removing local files associated to your cluster"
 rm -rf $CLUSTER_NAME
 rm -rf aws_launch_configuration_*.${CLUSTER_NAME}_user_data
}

function clean_route53_records() {
 echo "$(t1me) Removing stale k8s DNS entries from hosted zone "
 #This implantation could have been much simpler if only if aws cli parsed the json from console
 #Utility jq can make this much simpler

 records=($(aws route53 list-resource-record-sets  --hosted-zone-id $ROUTE53_ZONEID --query \
 'ResourceRecordSets[?ends_with(Name,`'"$CLUSTER_NAME"'.`) == `true`].Name[]' --output text))
 for rs in "${records[@]:-unset}";
   do
   	rr=$(aws route53 list-resource-record-sets  --hosted-zone-id $ROUTE53_ZONEID \
   	--query 'ResourceRecordSets[?Name == `'"$rs"'`].ResourceRecords[]')
   	aws route53 change-resource-record-sets --hosted-zone-id $ROUTE53_ZONEID \
   	--change-batch '{"Changes":[{"Action":"DELETE","ResourceRecordSet": {"ResourceRecords": '"$rr"',"Type":"A","Name":"'"$rs"'","TTL":60} }]}' \
   	|| echo "nothing to delete"
  done
}

function clean_tags() {
 #future implementation. remove tags added by add_subnet_tags function during creation
 echo "please login to console to remove the public subnet tags with key KubernetesCluster"
}  

if [ "${1:-unset}" == "unset" ] ; then
 echo "script requires valid options to proceed........"
 printhelp
fi

while getopts ":h" opt; do
	case ${opt} in
	h )
  	printhelp
	;;
	\? )
	echo "Invalid Option: -$OPTARG" 1>&2
	printhelp
	;;
	esac
done
shift $((OPTIND -1))
# Remove 'pekops' from the argument list
subcommand=${1:-unset}; shift
case "$subcommand" in
  # Parse options to the install sub command
  setup)
    # Process package options
    while getopts ":c:z:v:m:n:q:o:i:e:s:r:x:y:p:t:d:a:b:f:" opt; do
    	case ${opt} in
        c )
          CLUSTER_NAME=$OPTARG
          ;;
        z )
          ZONES=$OPTARG
          ;;
        v )
          VPC=$OPTARG
          ;;
        m )
          MASTER_COUNT=$OPTARG
          ;;
        n )
          NODE_COUNT_MAX=$OPTARG
          ;;
        q )
          NODE_COUNT_MIN=$OPTARG
          ;;
        o )
          OS_TYPE=$OPTARG
          ;;
        i )
          AMI_IMAGE_FILTER=$OPTARG
          ;; 
        f )
          AMI_OWNER=$OPTARG
          ;;
        e )
          EGRESS_PROXY=$OPTARG
          ;;
        s )
          S3_STORE_BUCKET_NAME=$OPTARG
          ;;
        r )
          ROUTE53_ZONEID=$OPTARG
          ;;
        x )
          MASTER_INSTANCE=$OPTARG
          ;;
        y )
          NODE_INSTANCE=$OPTARG
          ;;
        p )
          AWS_PROFILE=$OPTARG
          ;;
        t )
          TAG_NAME_IDENTIFIER=$OPTARG
          ;;
        d )
          SELF_ID=$OPTARG
          ;;
        a )
          PUBLIC_SUBNETS=$OPTARG
          ;;
        b )
          PRIVATE_SUBNETS=$OPTARG
          ;;
        \? )
          echo "Invalid Option -$OPTARG for $subcommand" 1>&2
          printhelp
          ;;
        : )
          echo "Invalid Option: -$OPTARG requires an argument" 1>&2
          printhelp
          ;;
        esac
    done
    shift $((OPTIND -1))
    ;;
   clean)
     while getopts ":c:s:r:p:z:" opt; do
     	case ${opt} in
        c )
          CLUSTER_NAME=$OPTARG
          ;;
        s )
          S3_STORE_BUCKET_NAME=$OPTARG
          ;;
        r )
          ROUTE53_ZONEID=$OPTARG
          ;;
        p )
          AWS_PROFILE=$OPTARG
          ;;
        z )
          ZONES=$OPTARG
          ;;
        \? )
          echo "Invalid Option -$OPTARG for $subcommand" 1>&2
          printhelp
          ;;
        : )
          echo "Invalid Option: -$OPTARG requires an argument" 1>&2
          printhelp
          ;;
        esac
    done
    shift $((OPTIND -1))
    ;;
esac

if [ "$subcommand" == "setup" ]; then
	echo "executing pekops $subcommand with below config"
	[[ ${CLUSTER_NAME:-unset} == "unset" ]] && printf "cluster name can't be null\n $(printhelp)" && exit 1 
	[[ ${CLUSTER_NAME:-unset} != "unset" ]] && printf "Cluster name: $CLUSTER_NAME \n"
	[[ ${VPC:-unset} == "unset" ]] && printf "vpc id can't be null\n $(printhelp)" && exit 1
	[[ ${VPC:-unset} != "unset" ]] && printf "vpc id: $VPC \n"
	[[ ${S3_STORE_BUCKET_NAME:-unset} == "unset" ]] && printf "S3 bucket name can't be null\n $(printhelp)" && exit 1 
	[[ ${S3_STORE_BUCKET_NAME:-unset} != "unset" ]] && printf "S3 bucket name: $S3_STORE_BUCKET_NAME \n"
	[[ ${ROUTE53_ZONEID:-unset} == "unset" ]] && printf "Route53 Zone name can't be null\n $(printhelp)" && exit 1
	[[ ${ROUTE53_ZONEID:-unset} != "unset" ]] && printf "Route53 Zone name: $ROUTE53_ZONEID \n"
        [[ ${EGRESS_PROXY:-unset} == "unset" ]] && printf "Egress proxy can't be null\n $(printhelp)" && exit 1
        [[ ${EGRESS_PROXY:-unset} != "unset" ]] && printf "Egress is: $EGRESS_PROXY \n"
	[[ ${OS_TYPE:-unset} == "unset" ]] && OS_TYPE=ubuntu
	[[ ${OS_TYPE:-unset} == "ubuntu" ]] && printf "OS type is : $OS_TYPE \n"
	[[ ${OS_TYPE:-unset} == "amzn2" ]] && printf "OS type is : $OS_TYPE \n"
	[[ ${OS_TYPE:-unset} != "unset" ]] && [[ ${OS_TYPE:-unset} != "amzn2" ]] && [[ ${OS_TYPE:-unset} != "ubuntu" ]] \
	&& printf "wrong os type \n $(printhelp)" && exit 1
        [[ ${AMI_OWNER:-unset} == "unset" ]] && AMI_OWNER=757541135089
        [[ ${AMI_OWNER:-unset} != "unset" ]] && printf "AMI Owner is : $AMI_OWNER \n"
	[[ ${AMI_IMAGE_FILTER:-unset} == "unset" ]] && AMI_IMAGE_FILTER="*"
	[[ ${AMI_IMAGE_FILTER:-unset} != "unset" ]] && printf "AMI filter is: "$AMI_IMAGE_FILTER" \n"
	[[ ${MASTER_COUNT:-unset} == "unset" ]] &&  MASTER_COUNT=3
	[[ ${MASTER_COUNT:-unset} != "unset" ]] && printf "Master node count for your cluster: $MASTER_COUNT \n"
	[[ ${NODE_COUNT_MAX:-unset} == "unset" ]] &&  NODE_COUNT_MAX=3
	[[ ${NODE_COUNT_MAX:-unset} != "unset" ]] && printf "Max worker node count for your cluster: $NODE_COUNT_MAX \n"
        [[ ${NODE_COUNT_MIN:-unset} == "unset" ]] &&  NODE_COUNT_MIN=3
        [[ ${NODE_COUNT_MIN:-unset} != "unset" ]] && printf "Min worker node count for your cluster: $NODE_COUNT_MIN \n"
	[[ ${MASTER_INSTANCE:-unset} == "unset" ]] && MASTER_INSTANCE=t2.medium
	[[ ${MASTER_INSTANCE:-unset} != "unset" ]] && printf "Master node type is : $MASTER_INSTANCE \n"
	[[ ${NODE_INSTANCE:-unset} == "unset" ]] && NODE_INSTANCE=t2.medium
	[[ ${NODE_INSTANCE:-unset} != "unset" ]] && printf "Master node type is : $NODE_INSTANCE \n"
	[[ ${ZONES:-unset} == "unset" ]] &&  ZONES=us-east-1a,us-east-1b,us-east-1c
	[[ ${ZONES:-unset} != "unset" ]] && printf "Availability zones for your cluster : $ZONES \n"
        [[ ${PUBLIC_SUBNETS:-unset} == "unset" ]] && printf "Public subnets can't be null\n $(printhelp)" && exit 1
        [[ ${PUBLIC_SUBNETS:-unset} != "unset" ]] && printf "Public subnets for your cluster: $PUBLIC_SUBNETS \n"
        [[ ${PRIVATE_SUBNETS:-unset} == "unset" ]] && printf "Private subnets can't be null\n $(printhelp)" && exit 1
        [[ ${PRIVATE_SUBNETS:-unset} != "unset" ]] && printf "Private subnets for your cluster: $PRIVATE_SUBNETS \n"
	[[ ${AWS_PROFILE:-unset} == "unset" ]] && AWS_PROFILE=default
	[[ ${AWS_PROFILE:-unset} != "unset" ]] && printf "AWS credentials for your cluster is : $AWS_PROFILE \n"
	[[ ${TAG_NAME_IDENTIFIER:-unset} == "unset" ]] && TAG_NAME_IDENTIFIER=pe
	[[ ${TAG_NAME_IDENTIFIER:-unset} != "unset" ]] && printf "Tag name identifier for your cluster is : $TAG_NAME_IDENTIFIER \n"
        [[ ${SELF_ID:-unset} == "unset" ]] && SELF_ID=""
        [[ ${SELF_ID:-unset} != "unset" ]] && printf "unique id is $SELF_ID \n"
	AWS_ZONES=(${ZONES//,/ })
        AWS_PUBLIC_SUBNETS=(${PUBLIC_SUBNETS//,/ })
        AWS_PRIVATE_SUBNETS=(${PRIVATE_SUBNETS//,/ })
        export AWS_DEFAULT_REGION=${AWS_ZONES[0]:0:${#AWS_ZONES[0]}-1}
        export AWS_PROFILE=$AWS_PROFILE
	##create assests
	#validate_provisioner_policy //doesn't work when federation is involved 
	validate_vpc
	get_az_list
	get_etcd_map
	get_etcd_list
        get_subnet_list
        get_vpc_cidr
        validate_ami
	create_launch_script_master
	create_launch_script_nodes
	manage_addons
	manage_instancegroup_master
	manage_instancegroup_nodes
	manage_pki
	manage_secrets
	manage_kubeconfig
	manage_cluster_spec
        #update_terraform_user_config
	##create s3 bucket
	create_s3_bucket_and_upload_assests
	add_subnet_tags
elif [ "$subcommand" == "clean" ]; then
	echo "executing pekops $subcommand with below config"
	[[ ${CLUSTER_NAME:-unset} == "unset" ]] && printf "cluster name can't be null\n $(printhelp)" && exit 1
	[[ ${CLUSTER_NAME:-unset} != "unset" ]] && printf "Cluster name: $CLUSTER_NAME \n"
	[[ ${S3_STORE_BUCKET_NAME:-unset} == "unset" ]] && printf "S3 bucket name can't be null\n $(printhelp)" && exit 1
	[[ ${S3_STORE_BUCKET_NAME:-unset} != "unset" ]] && printf "S3 bucket name: $S3_STORE_BUCKET_NAME \n"
	[[ ${ROUTE53_ZONEID:-unset} == "unset" ]] && printf "Route53 Zone name can't be null\n $(printhelp)" && exit 1
	[[ ${ROUTE53_ZONEID:-unset} != "unset" ]] && printf "Route53 Zone name : $ROUTE53_ZONEID \n"
	[[ ${AWS_PROFILE:-unset} == "unset" ]] && AWS_PROFILE=default
	[[ ${AWS_PROFILE:-unset} != "unset" ]] && printf "AWS credentials for your cluster is : $AWS_PROFILE \n"
        [[ ${ZONES:-unset} == "unset" ]] &&  ZONES=us-east-1a,us-east-1b,us-east-1c
        [[ ${ZONES:-unset} != "unset" ]] && printf "Availability zones for your cluster : $ZONES \n"
        AWS_ZONES=(${ZONES//,/ })
        export AWS_DEFAULT_REGION=${AWS_ZONES[0]:0:${#AWS_ZONES[0]}-1}
        export AWS_PROFILE=$AWS_PROFILE
	##clean
	clean_s3_bucket
	#clean_workspace
	clean_route53_records
	clean_tags
else
	echo "$subcommand is not a valid option"
	printhelp
fi
